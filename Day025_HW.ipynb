{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day025_HW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf7pbKomT3hN",
        "colab_type": "text"
      },
      "source": [
        "![Scrapy word-flow](http://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abjXcP1QSln7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install Scrapy\n",
        "# !curl -o myfile.zip \"http://pycrawler.cupoy.com/file-download/part26/Day026_Scrapy_init.zip\"\n",
        "# !unzip myfile.zip -d \"./\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONvDo_BW4kF6",
        "colab_type": "code",
        "outputId": "78d39091-ca50-4485-d12c-07b98ccbcda5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# !scrapy startproject myproject\n",
        "# !scrapy genspider PTTCrawler www.ptt.cc\n",
        "# !python ./PTTCrawler.py\n",
        "!cat ./PTTCrawler.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "import scrapy\n",
            "import re\n",
            "from bs4 import BeautifulSoup\n",
            "from urllib.parse import urljoin\n",
            "from pprint import pprint\n",
            "\n",
            "# 範例目標網址: https://www.ptt.cc/bbs/Gossiping/M.1557928779.A.0C1.html\n",
            "class PttcrawlerSpider(scrapy.Spider):\n",
            "    name = 'PTTCrawler'\n",
            "    allowed_domains = ['www.ptt.cc']\n",
            "    start_urls = ['https://www.ptt.cc/bbs/Gossiping/M.1557928779.A.0C1.html']\n",
            "    cookies = {'over18': '1'}\n",
            "\n",
            "    def start_requests(self):\n",
            "        for url in self.start_urls:\n",
            "            yield scrapy.Request(url=url, callback=self.parse, cookies=self.cookies)\n",
            "\n",
            "    def parse(self, response):\n",
            "        # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
            "        if response.status != 200:\n",
            "            print('Error - {} is not available to access'.format(response.url))\n",
            "            return\n",
            "\n",
            "        # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
            "        soup = BeautifulSoup(response.text)\n",
            "\n",
            "        \n",
            "        # 取得文章內容主體\n",
            "        main_content = soup.find(id='main-content')\n",
            "        \n",
            "        # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
            "        metas = main_content.select('div.article-metaline')\n",
            "        author = ''\n",
            "        title = ''\n",
            "        date = ''\n",
            "        if metas:\n",
            "            if metas[0].select('span.article-meta-value')[0]:\n",
            "                author = metas[0].select('span.article-meta-value')[0].string\n",
            "            if metas[1].select('span.article-meta-value')[0]:\n",
            "                title = metas[1].select('span.article-meta-value')[0].string\n",
            "            if metas[2].select('span.article-meta-value')[0]:\n",
            "                date = metas[2].select('span.article-meta-value')[0].string\n",
            "\n",
            "            # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
            "            #\n",
            "            # .extract() 方法可以參考官方文件\n",
            "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
            "            for m in metas:\n",
            "                m.extract()\n",
            "            for m in main_content.select('div.article-metaline-right'):\n",
            "                m.extract()\n",
            "        \n",
            "        # 取得留言區主體\n",
            "        pushes = main_content.find_all('div', class_='push')\n",
            "        for p in pushes:\n",
            "            p.extract()\n",
            "        \n",
            "        # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
            "        # 透過 regular expression 取得 IP\n",
            "        # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
            "        try:\n",
            "            ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
            "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
            "        except Exception as e:\n",
            "            ip = ''\n",
            "        \n",
            "        # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
            "        # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
            "        #\n",
            "        # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
            "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
            "        filtered = []\n",
            "        for v in main_content.stripped_strings:\n",
            "            # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
            "            if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
            "                filtered.append(v)\n",
            "\n",
            "        # 定義一些特殊符號與全形符號的過濾器\n",
            "        expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
            "        for i in range(len(filtered)):\n",
            "            filtered[i] = re.sub(expr, '', filtered[i])\n",
            "        \n",
            "        # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
            "        filtered = [i for i in filtered if i]\n",
            "        content = ' '.join(filtered)\n",
            "        \n",
            "        # 處理留言區\n",
            "        # p 計算推文數量\n",
            "        # b 計算噓文數量\n",
            "        # n 計算箭頭數量\n",
            "        p, b, n = 0, 0, 0\n",
            "        messages = []\n",
            "        for push in pushes:\n",
            "            # 假如留言段落沒有 push-tag 就跳過\n",
            "            if not push.find('span', 'push-tag'):\n",
            "                continue\n",
            "            \n",
            "            # 過濾額外空白與換行符號\n",
            "            # push_tag 判斷是推文, 箭頭還是噓文\n",
            "            # push_userid 判斷留言的人是誰\n",
            "            # push_content 判斷留言內容\n",
            "            # push_ipdatetime 判斷留言日期時間\n",
            "            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
            "            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
            "            push_content = push.find('span', 'push-content').strings\n",
            "            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
            "            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
            "\n",
            "            # 整理打包留言的資訊, 並統計推噓文數量\n",
            "            messages.append({\n",
            "                'push_tag': push_tag,\n",
            "                'push_userid': push_userid,\n",
            "                'push_content': push_content,\n",
            "                'push_ipdatetime': push_ipdatetime})\n",
            "            if push_tag == u'推':\n",
            "                p += 1\n",
            "            elif push_tag == u'噓':\n",
            "                b += 1\n",
            "            else:\n",
            "                n += 1\n",
            "        \n",
            "        # 統計推噓文\n",
            "        # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
            "        # all 為總共留言數量 \n",
            "        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
            "        \n",
            "        # 整理文章資訊\n",
            "        data = {\n",
            "            'url': response.url,\n",
            "            'article_author': author,\n",
            "            'article_title': title,\n",
            "            'article_date': date,\n",
            "            'article_content': content,\n",
            "            'ip': ip,\n",
            "            'message_count': message_count,\n",
            "            'messages': messages\n",
            "        }\n",
            "        yield data\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}