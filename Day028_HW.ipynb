{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day028_HW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIjHJea9Es6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 安裝 Scrapy\n",
        "# !pip install Scrapy\n",
        "\n",
        "# 下載資料檔\n",
        "# !curl -o myfile.zip \"http://pycrawler.cupoy.com/file-download/part28/Day028_Scrapy_API.zip\"\n",
        "# !unzip myfile.zip -d \"./\"\n",
        "# !rm myfile.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Fv16GtKdBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 建立 Scrapy project\n",
        "\n",
        "# !scrapy startproject myproject ./\n",
        "# !scrapy genspider PTTCrawler www.ptt.cc\n",
        "# !scrapy list\n",
        "\n",
        "# !scrapy crawl PTTCrawler "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mia0DDQN_N-L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b08f8b2-e5f9-4d11-cd7c-db1b8e9bc1f6"
      },
      "source": [
        "!python './myproject/main.py'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-10 06:18:11 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: myproject)\n",
            "2020-01-10 06:18:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.6.9 (default, Nov  7 2019, 10:44:02) - [GCC 8.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-01-10 06:18:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'myproject', 'NEWSPIDER_MODULE': 'myproject.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['myproject.spiders']}\n",
            "2020-01-10 06:18:11 [scrapy.extensions.telnet] INFO: Telnet Password: 3289b9a4da43790b\n",
            "2020-01-10 06:18:11 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-01-10 06:18:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-01-10 06:18:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-01-10 06:18:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "['myproject.pipelines.JSONPipeline']\n",
            "2020-01-10 06:18:11 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-01-10 06:18:11 [PTTCrawler] DEBUG: Create temp file for store JSON - /content/crawled_data/.tmp.json.swp\n",
            "2020-01-10 06:18:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-01-10 06:18:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2020-01-10 06:18:11 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.ptt.cc/robots.txt> (referer: None)\n",
            "2020-01-10 06:18:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ptt.cc/bbs/Gossiping/M.1557928779.A.0C1.html> (referer: None)\n",
            "/content/myproject/spiders/PTTCrawler.py:29: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 29 of the file /content/myproject/spiders/PTTCrawler.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  soup = BeautifulSoup(response.text)\n",
            "2020-01-10 06:18:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.ptt.cc/bbs/Gossiping/M.1557928779.A.0C1.html>\n",
            "{'url': 'https://www.ptt.cc/bbs/Gossiping/M.1557928779.A.0C1.html', 'article_id': 'M.1557928779.A.0C1', 'article_author': 'hahaccu (靠臉吃飯的小魯)', 'article_title': '[問卦] 請家人吃飯好店推薦', 'article_date': 'Wed May 15 21:59:37 2019', 'article_content': '大家好，台嘎後，打給後\\n\\n本肥請家人吃飯\\n\\n每次只能想到欣葉，馬辣等吃到飽沒新意的店\\n\\n有沒有厲害有特色的店可以推薦\\n\\n謝謝\\n\\n https://www.ptt.cc/bbs/Gossiping/M.1557928779.A.0C1.html 對，北部 味道不是都一樣ㄇ (筆記)', 'ip': '111.71.55.130', 'message_count': {'all': 12, 'count': 0, 'push': 2, 'boo': 2, 'neutral': 8}, 'messages': [{'push_tag': '→', 'push_userid': 'uuyouru', 'push_content': '住哪', 'push_ipdatetime': '05/15 21:59'}, {'push_tag': '噓', 'push_userid': 'DsLove710', 'push_content': '麥當勞', 'push_ipdatetime': '05/15 21:59'}, {'push_tag': '→', 'push_userid': 'uuyouru', 'push_content': '台北ㄇ', 'push_ipdatetime': '05/15 22:00'}, {'push_tag': '→', 'push_userid': 'hawaii987', 'push_content': '你家巷口', 'push_ipdatetime': '05/15 22:00'}, {'push_tag': '噓', 'push_userid': 'Doub1eK', 'push_content': '我家巷口', 'push_ipdatetime': '05/15 22:00'}, {'push_tag': '推', 'push_userid': 'pipi8696044', 'push_content': '台科大自助餐', 'push_ipdatetime': '05/15 22:00'}, {'push_tag': '→', 'push_userid': 'tyrande', 'push_content': 'M記快餐 K記快餐', 'push_ipdatetime': '05/15 22:00'}, {'push_tag': '→', 'push_userid': 'DongWooNeKo', 'push_content': '不知道', 'push_ipdatetime': '05/15 22:00'}, {'push_tag': '→', 'push_userid': 'JEWH', 'push_content': '女權', 'push_ipdatetime': '05/15 22:00'}, {'push_tag': '→', 'push_userid': 'guhong', 'push_content': '中壢家樂福一樓的三商巧福，好吃到舌頭掉下來', 'push_ipdatetime': '05/15 22:01'}, {'push_tag': '→', 'push_userid': 'alex00089', 'push_content': '墾丁嗎？', 'push_ipdatetime': '05/15 22:04'}, {'push_tag': '推', 'push_userid': 'boboking2', 'push_content': '新馬辣', 'push_ipdatetime': '05/15 22:05'}]}\n",
            "2020-01-10 06:18:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ptt.cc/bbs/Gossiping/M.1559788476.A.074.html> (referer: None)\n",
            "2020-01-10 06:18:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.ptt.cc/bbs/Gossiping/M.1559788476.A.074.html>\n",
            "{'url': 'https://www.ptt.cc/bbs/Gossiping/M.1559788476.A.074.html', 'article_id': 'M.1559788476.A.074', 'article_author': 'STAV72 (刁民黨黨務主委)', 'article_title': '[問卦] 新疆爆發禽流感 總撲殺數破萬', 'article_date': 'Thu Jun  6 10:34:34 2019', 'article_content': 'https://i.imgur.com/ZDkP38v.jpg https://i.imgur.com/rhToRIS.jpg 2019年5月30日，農業農村部接到動物疫病預防控制中心報告，經國家禽流感參考實驗室\\n確診，伊犁州霍爾果斯市部分養殖戶飼養的家禽發生H5N6亞型高致病性禽流感疫情。相\\n關養殖戶共存欄家禽2445羽，發病1503羽，死亡1015余羽。疫情發生後，當地按照有關\\n預案和防治技術規範要求，切實做好疫情處置工作，已 撲殺家禽11910羽 ，全部病死和撲\\n殺家禽均已無害化處理。\\n\\n\\nSent from BePTT https://www.ptt.cc/bbs/Gossiping/M.1559788476.A.074.html 什麼時候可以貼港澳新聞了，\\n只能爆卦啊？連新唐人跟大紀元都沒報，\\n去哪生臺灣新聞？\\n\\n三明治也沒有，只能爆卦啊？', 'ip': '49.216.209.175', 'message_count': {'all': 32, 'count': 7, 'push': 12, 'boo': 5, 'neutral': 15}, 'messages': [{'push_tag': '→', 'push_userid': 's820912gmail', 'push_content': '韓流', 'push_ipdatetime': '06/06 10:34'}, {'push_tag': '噓', 'push_userid': 'kent', 'push_content': '新聞改爆卦喔 我國三洨', 'push_ipdatetime': '06/06 10:35'}, {'push_tag': '→', 'push_userid': 'linceass', 'push_content': '沒牛雞吃了 只能吃豬', 'push_ipdatetime': '06/06 10:35'}, {'push_tag': '→', 'push_userid': 'Kreen', 'push_content': '讓中國人吃下肚消滅病原～', 'push_ipdatetime': '06/06 10:35'}, {'push_tag': '推', 'push_userid': 'dbdudsorj', 'push_content': '吃草吧', 'push_ipdatetime': '06/06 10:35'}, {'push_tag': '推', 'push_userid': 'mack860120', 'push_content': '習下韓上', 'push_ipdatetime': '06/06 10:35'}, {'push_tag': '噓', 'push_userid': 'no4', 'push_content': '', 'push_ipdatetime': '06/06 10:35'}, {'push_tag': '→', 'push_userid': 'linceass', 'push_content': '88你違反板龜了', 'push_ipdatetime': '06/06 10:35'}, {'push_tag': '推', 'push_userid': 'coollonger', 'push_content': '連鳥也沒得吃了嗎', 'push_ipdatetime': '06/06 10:35'}, {'push_tag': '→', 'push_userid': 'jorden', 'push_content': '豬雞都生病嘞 接下來換牛羊了 塊陶阿 天譴來了', 'push_ipdatetime': '06/06 10:35'}, {'push_tag': '噓', 'push_userid': 'pigpig1103', 'push_content': '這種事還要講？沒事啦幹', 'push_ipdatetime': '06/06 10:36'}, {'push_tag': '→', 'push_userid': 'henry55566', 'push_content': '韓流發威', 'push_ipdatetime': '06/06 10:36'}, {'push_tag': '推', 'push_userid': 'cores', 'push_content': '昨天是牛，今天換家禽，中國怎麼了，這樣怎麼中或贏啦XD', 'push_ipdatetime': '06/06 10:37'}, {'push_tag': '推', 'push_userid': 'kyowinner', 'push_content': '人都可以撲殺了何況家禽', 'push_ipdatetime': '06/06 10:37'}, {'push_tag': '→', 'push_userid': 'echo3283', 'push_content': '中共可以吃吐吃草 怕什麼', 'push_ipdatetime': '06/06 10:38'}, {'push_tag': '→', 'push_userid': 'hbj1941', 'push_content': '整體可控，沒4', 'push_ipdatetime': '06/06 10:38'}, {'push_tag': '→', 'push_userid': 'Ipadhotwater', 'push_content': '這種爆發狀況中國的確是贏家', 'push_ipdatetime': '06/06 10:38'}, {'push_tag': '推', 'push_userid': 'bigbirdfly', 'push_content': '嚴重懷疑中共被投放病毒……怎麼這幾年狂得病', 'push_ipdatetime': '06/06 10:39'}, {'push_tag': '→', 'push_userid': 'beyoursky', 'push_content': '台灣如果對自己有信心 就不用怕引進中國雞肉', 'push_ipdatetime': '06/06 10:39'}, {'push_tag': '推', 'push_userid': 'xxxg00w0', 'push_content': '........豬 牛 接下來換雞鴨...剩羊了嗎= =\"', 'push_ipdatetime': '06/06 10:40'}, {'push_tag': '噓', 'push_userid': 'kent', 'push_content': '我國是三洨啊', 'push_ipdatetime': '06/06 10:40'}, {'push_tag': '推', 'push_userid': 'color3258', 'push_content': '中國大煉蠱', 'push_ipdatetime': '06/06 10:40'}, {'push_tag': '→', 'push_userid': 'cores', 'push_content': '美國生物技術很強，貿易戰上要加大中國困局，是有動機的', 'push_ipdatetime': '06/06 10:40'}, {'push_tag': '推', 'push_userid': 'color3258', 'push_content': '中國有羊炭疽啊', 'push_ipdatetime': '06/06 10:42'}, {'push_tag': '→', 'push_userid': 'cores', 'push_content': '非洲豬瘟、秋行軍蟲、牛口蹄疫、羊碳疽、雞禽流感 很慘', 'push_ipdatetime': '06/06 10:43'}, {'push_tag': '噓', 'push_userid': 'yniori', 'push_content': '你的國家好爛喔~一直在死雞死牛死豬~~', 'push_ipdatetime': '06/06 10:43'}, {'push_tag': '→', 'push_userid': 'yniori', 'push_content': '但不管怎樣~都不會死超過35人喔!!', 'push_ipdatetime': '06/06 10:43'}, {'push_tag': '→', 'push_userid': 'STAV72', 'push_content': '修一下好了，不然感覺到吵個沒完。', 'push_ipdatetime': '06/06 10:44'}, {'push_tag': '→', 'push_userid': 'yu1111116', 'push_content': '哇靠！ 這是在收集解成就喔', 'push_ipdatetime': '06/06 10:44'}, {'push_tag': '推', 'push_userid': 'tamynumber1', 'push_content': '中國現在還有什麼傳染病還沒解鎖成就的?', 'push_ipdatetime': '06/06 10:47'}, {'push_tag': '推', 'push_userid': 'bbbyy', 'push_content': '阿六病死照吃 禽流感沒影響', 'push_ipdatetime': '06/06 10:58'}, {'push_tag': '推', 'push_userid': 'insist0511', 'push_content': '還有草能吃 沒事兒沒事兒', 'push_ipdatetime': '06/06 11:33'}]}\n",
            "2020-01-10 06:18:12 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2020-01-10 06:18:12 [PTTCrawler] DEBUG: Save result at /content/crawled_data/test.json\n",
            "2020-01-10 06:18:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 855,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 7513,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 1.36299,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2020, 1, 10, 6, 18, 12, 980677),\n",
            " 'item_scraped_count': 2,\n",
            " 'log_count/DEBUG': 7,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 94924800,\n",
            " 'memusage/startup': 94924800,\n",
            " 'response_received_count': 3,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 2,\n",
            " 'scheduler/dequeued/memory': 2,\n",
            " 'scheduler/enqueued': 2,\n",
            " 'scheduler/enqueued/memory': 2,\n",
            " 'start_time': datetime.datetime(2020, 1, 10, 6, 18, 11, 617687)}\n",
            "2020-01-10 06:18:12 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Af3chYClOq",
        "colab_type": "text"
      },
      "source": [
        "建立 project 後，須調整部分如下\n",
        "\n",
        "1.新增 PTTCrawler.py\n",
        "\n",
        "2.修改 items.py\n",
        "\n",
        "3.修改 pipelines.py\n",
        "\n",
        "4.修改 settings.py\n",
        "\n",
        "5.新增 main.py\n",
        "\n",
        "執行!python '/***/main.py'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IletGTFGPCXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''新增 PTTCrawler.py'''\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "from ..items import PTTArticleItem\n",
        "\n",
        "class PttcrawlerSpider(scrapy.Spider):\n",
        "    name = 'PTTCrawler'\n",
        "    def __init__(self, start_urls, filename=None):\n",
        "        self.cookies = {'over18': '1'}\n",
        "        self.start_urls = start_urls\n",
        "        self.filename = filename\n",
        "        super().__init__()\n",
        "\n",
        "    def start_requests(self):\n",
        "        for url in self.start_urls:\n",
        "            yield scrapy.Request(url=url, callback=self.parse, cookies=self.cookies)\n",
        "\n",
        "    def parse(self, response):\n",
        "        # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
        "        if response.status != 200:\n",
        "            print('Error - {} is not available to access'.format(response.url))\n",
        "            return\n",
        "\n",
        "        # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
        "        soup = BeautifulSoup(response.text)\n",
        "\n",
        "        \n",
        "        # 取得文章內容主體\n",
        "        main_content = soup.find(id='main-content')\n",
        "        \n",
        "        # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
        "        metas = main_content.select('div.article-metaline')\n",
        "        author = ''\n",
        "        title = ''\n",
        "        date = ''\n",
        "        if metas:\n",
        "            if metas[0].select('span.article-meta-value')[0]:\n",
        "                author = metas[0].select('span.article-meta-value')[0].string\n",
        "            if metas[1].select('span.article-meta-value')[0]:\n",
        "                title = metas[1].select('span.article-meta-value')[0].string\n",
        "            if metas[2].select('span.article-meta-value')[0]:\n",
        "                date = metas[2].select('span.article-meta-value')[0].string\n",
        "\n",
        "            # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
        "            #\n",
        "            # .extract() 方法可以參考官方文件\n",
        "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
        "            for m in metas:\n",
        "                m.extract()\n",
        "            for m in main_content.select('div.article-metaline-right'):\n",
        "                m.extract()\n",
        "        \n",
        "        # 取得留言區主體\n",
        "        pushes = main_content.find_all('div', class_='push')\n",
        "        for p in pushes:\n",
        "            p.extract()\n",
        "        \n",
        "        # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
        "        # 透過 regular expression 取得 IP\n",
        "        # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
        "        try:\n",
        "            ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
        "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
        "        except Exception as e:\n",
        "            ip = ''\n",
        "        \n",
        "        # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
        "        # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
        "        #\n",
        "        # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
        "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
        "        filtered = []\n",
        "        for v in main_content.stripped_strings:\n",
        "            # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
        "            if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
        "                filtered.append(v)\n",
        "\n",
        "        # 定義一些特殊符號與全形符號的過濾器\n",
        "        expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
        "        for i in range(len(filtered)):\n",
        "            filtered[i] = re.sub(expr, '', filtered[i])\n",
        "        \n",
        "        # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
        "        filtered = [i for i in filtered if i]\n",
        "        content = ' '.join(filtered)\n",
        "        \n",
        "        # 處理留言區\n",
        "        # p 計算推文數量\n",
        "        # b 計算噓文數量\n",
        "        # n 計算箭頭數量\n",
        "        p, b, n = 0, 0, 0\n",
        "        messages = []\n",
        "        for push in pushes:\n",
        "            # 假如留言段落沒有 push-tag 就跳過\n",
        "            if not push.find('span', 'push-tag'):\n",
        "                continue\n",
        "            \n",
        "            # 過濾額外空白與換行符號\n",
        "            # push_tag 判斷是推文, 箭頭還是噓文\n",
        "            # push_userid 判斷留言的人是誰\n",
        "            # push_content 判斷留言內容\n",
        "            # push_ipdatetime 判斷留言日期時間\n",
        "            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
        "            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
        "            push_content = push.find('span', 'push-content').strings\n",
        "            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
        "            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
        "\n",
        "            # 整理打包留言的資訊, 並統計推噓文數量\n",
        "            messages.append({\n",
        "                'push_tag': push_tag,\n",
        "                'push_userid': push_userid,\n",
        "                'push_content': push_content,\n",
        "                'push_ipdatetime': push_ipdatetime})\n",
        "            if push_tag == u'推':\n",
        "                p += 1\n",
        "            elif push_tag == u'噓':\n",
        "                b += 1\n",
        "            else:\n",
        "                n += 1\n",
        "        \n",
        "        # 統計推噓文\n",
        "        # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
        "        # all 為總共留言數量 \n",
        "        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
        "        \n",
        "        # 整理文章資訊\n",
        "        data = PTTArticleItem()\n",
        "        article_id = str(Path(urlparse(response.url).path).stem)\n",
        "        data['url'] = response.url\n",
        "        data['article_id'] = article_id\n",
        "        data['article_author'] = author\n",
        "        data['article_title'] = title\n",
        "        data['article_date'] = date\n",
        "        data['article_content'] = content\n",
        "        data['ip'] = ip\n",
        "        data['message_count'] = message_count\n",
        "        data['messages'] = messages\n",
        "        yield data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuXU9Vs5Arfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''修改 items.py'''\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Define here the models for your scraped items\n",
        "#\n",
        "# See documentation in:\n",
        "# https://doc.scrapy.org/en/latest/topics/items.html\n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class PTTArticleItem(scrapy.Item):\n",
        "    url = scrapy.Field()\n",
        "    article_id = scrapy.Field()\n",
        "    article_author = scrapy.Field()\n",
        "    article_title = scrapy.Field()\n",
        "    article_date = scrapy.Field()\n",
        "    article_content = scrapy.Field()\n",
        "    ip = scrapy.Field()\n",
        "    message_count = scrapy.Field()\n",
        "    messages = scrapy.Field()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXddX4A4PPZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''修改 pipelines.py'''\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Define your item pipelines here\n",
        "#\n",
        "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
        "# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "import os\n",
        "import json\n",
        "\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "class MyprojectPipeline(object):\n",
        "    def process_item(self, item, spider):\n",
        "        return item\n",
        "\n",
        "\n",
        "class JSONPipeline(object):\n",
        "    def open_spider(self, spider):\n",
        "        self.start_crawl_datetime = datetime.now().strftime('%Y%m%dT%H:%M:%S')\n",
        "\n",
        "        # 在開始爬蟲的時候建立暫時的 JSON 檔案\n",
        "        # 避免有多筆爬蟲結果的時候，途中發生錯誤導致程式停止會遺失所有檔案\n",
        "        self.dir_path = Path(__file__).resolve().parents[1] / 'crawled_data'\n",
        "        self.runtime_file_path = str(self.dir_path / '.tmp.json.swp')\n",
        "        if not self.dir_path.exists():\n",
        "            self.dir_path.mkdir(parents=True)\n",
        "        spider.log('Create temp file for store JSON - {}'.format(self.runtime_file_path))\n",
        "\n",
        "        # 設計 JSON 存的格式為\n",
        "        # [\n",
        "        #  {...}, # 一筆爬蟲結果\n",
        "        #  {...}, ...\n",
        "        # ]\n",
        "        self.runtime_file = open(self.runtime_file_path, 'w+', encoding='utf8')\n",
        "        self.runtime_file.write('[\\n')\n",
        "        self._first_item = True\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        # 把資料轉成字典格式並寫入文件中\n",
        "        if not isinstance(item, dict):\n",
        "            item = dict(item)\n",
        "\n",
        "        if self._first_item:\n",
        "            self._first_item = False\n",
        "        else:\n",
        "            self.runtime_file.write(',\\n')\n",
        "\n",
        "        self.runtime_file.write(json.dumps(item, ensure_ascii=False))\n",
        "        return item\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.end_crawl_datetime = datetime.now().strftime('%Y%m%dT%H:%M:%S')\n",
        "\n",
        "        # 儲存 JSON 格式\n",
        "        self.runtime_file.write('\\n]')\n",
        "        self.runtime_file.close()\n",
        "        \n",
        "        # 將暫存檔改為以日期為檔名的格式\n",
        "        self.store_file_path = self.dir_path / '{}-{}.json'.format(self.start_crawl_datetime,\n",
        "                                                                   self.end_crawl_datetime)\n",
        "        # 假如 PTT 爬蟲有給定存檔檔名，就使用給予的檔名\n",
        "        if spider.name == 'PTTCrawler' and spider.filename:\n",
        "            if Path(spider.filename).suffix == '.json':\n",
        "                self.store_file_path = self.dir_path / spider.filename\n",
        "            else:\n",
        "                self.store_file_path = self.dir_path / '{}.json'.format(spider.filename)\n",
        "\n",
        "        self.store_file_path = str(self.store_file_path)\n",
        "        os.rename(self.runtime_file_path, self.store_file_path)\n",
        "        spider.log('Save result at {}'.format(self.store_file_path))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8CR5KVxAdJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''修改 settings.py'''\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# Scrapy settings for myproject project\n",
        "#\n",
        "# For simplicity, this file contains only settings considered important or\n",
        "# commonly used. You can find more settings consulting the documentation:\n",
        "#\n",
        "#     https://doc.scrapy.org/en/latest/topics/settings.html\n",
        "#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "\n",
        "BOT_NAME = 'myproject'\n",
        "\n",
        "SPIDER_MODULES = ['myproject.spiders']\n",
        "NEWSPIDER_MODULE = 'myproject.spiders'\n",
        "\n",
        "\n",
        "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
        "#USER_AGENT = 'myproject (+http://www.yourdomain.com)'\n",
        "\n",
        "# Obey robots.txt rules\n",
        "ROBOTSTXT_OBEY = True\n",
        "\n",
        "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
        "#CONCURRENT_REQUESTS = 32\n",
        "\n",
        "# Configure a delay for requests for the same website (default: 0)\n",
        "# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay\n",
        "# See also autothrottle settings and docs\n",
        "#DOWNLOAD_DELAY = 3\n",
        "# The download delay setting will honor only one of:\n",
        "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
        "#CONCURRENT_REQUESTS_PER_IP = 16\n",
        "\n",
        "# Disable cookies (enabled by default)\n",
        "#COOKIES_ENABLED = False\n",
        "\n",
        "# Disable Telnet Console (enabled by default)\n",
        "#TELNETCONSOLE_ENABLED = False\n",
        "\n",
        "# Override the default request headers:\n",
        "#DEFAULT_REQUEST_HEADERS = {\n",
        "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "#   'Accept-Language': 'en',\n",
        "#}\n",
        "\n",
        "# Enable or disable spider middlewares\n",
        "# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "#SPIDER_MIDDLEWARES = {\n",
        "#    'myproject.middlewares.MyprojectSpiderMiddleware': 543,\n",
        "#}\n",
        "\n",
        "# Enable or disable downloader middlewares\n",
        "# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "#DOWNLOADER_MIDDLEWARES = {\n",
        "#    'myproject.middlewares.MyprojectDownloaderMiddleware': 543,\n",
        "#}\n",
        "\n",
        "# Enable or disable extensions\n",
        "# See https://doc.scrapy.org/en/latest/topics/extensions.html\n",
        "#EXTENSIONS = {\n",
        "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
        "#}\n",
        "\n",
        "# Configure item pipelines\n",
        "# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "ITEM_PIPELINES = {\n",
        "#    'myproject.pipelines.MyprojectPipeline': 300,\n",
        "    'myproject.pipelines.JSONPipeline': 10\n",
        "}\n",
        "\n",
        "# Enable and configure the AutoThrottle extension (disabled by default)\n",
        "# See https://doc.scrapy.org/en/latest/topics/autothrottle.html\n",
        "#AUTOTHROTTLE_ENABLED = True\n",
        "# The initial download delay\n",
        "#AUTOTHROTTLE_START_DELAY = 5\n",
        "# The maximum download delay to be set in case of high latencies\n",
        "#AUTOTHROTTLE_MAX_DELAY = 60\n",
        "# The average number of requests Scrapy should be sending in parallel to\n",
        "# each remote server\n",
        "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
        "# Enable showing throttling stats for every response received:\n",
        "#AUTOTHROTTLE_DEBUG = False\n",
        "\n",
        "# Enable and configure HTTP caching (disabled by default)\n",
        "# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
        "#HTTPCACHE_ENABLED = True\n",
        "#HTTPCACHE_EXPIRATION_SECS = 0\n",
        "#HTTPCACHE_DIR = 'httpcache'\n",
        "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
        "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWbecw7PB_nN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''新增 main.py'''\n",
        "\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "\n",
        "def main():\n",
        "    target_urls = [\n",
        "        'https://www.ptt.cc/bbs/Gossiping/M.1559788476.A.074.html',\n",
        "        'https://www.ptt.cc/bbs/Gossiping/M.1557928779.A.0C1.html'\n",
        "    ]\n",
        "    process = CrawlerProcess(get_project_settings())\n",
        "    process.crawl('PTTCrawler', start_urls=target_urls, filename='test.json')\n",
        "    process.start()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}